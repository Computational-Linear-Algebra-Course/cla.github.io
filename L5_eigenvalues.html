
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5. Finding eigenvalues of matrices &#8212; Computational linear algebra course 2020.0 documentation</title>
    <link rel="stylesheet" href="_static/fenics.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/proof.js"></script>
    <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="4. LU decomposition" href="L4_LU_decomposition.html" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

<link rel="stylesheet" href="_static/featured.css">


<link rel="shortcut icon" href="_static/icon.ico" />


  </head><body>
<div class="wrapper">
  <a href="index.html"><img src="_static/banner.png" width="900px" alt="Project Banner" /></a>
  <div id="access">
    <div class="menu">
      <ul>
          <li class="page_item"><a href="https://github.com/Computational-Linear-Algebra-Course/computational-linear-algebra-course" title="GitHub">GitHub</a></li>
      </ul>
    </div><!-- .menu -->
  </div><!-- #access -->
</div><!-- #wrapper -->


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="finding-eigenvalues-of-matrices">
<h1><span class="section-number">5. </span>Finding eigenvalues of matrices<a class="headerlink" href="#finding-eigenvalues-of-matrices" title="Permalink to this headline">¶</a></h1>
<p>We start with some preliminary terminology.  A vector <span class="math notranslate nohighlight">\(x\in
\mathbb{C}^m\)</span> is an <em>eigenvector</em> of a square matrix <span class="math notranslate nohighlight">\(A\in
\mathbb{C}^{m\times m}\)</span> with <em>eigenvalue</em> <span class="math notranslate nohighlight">\(\lambda\)</span> if <span class="math notranslate nohighlight">\(Ax=\lambda
x\)</span>. An eigenspace is the subspace <span class="math notranslate nohighlight">\(E_{\lambda}\subset\mathbb{C}^m\)</span>
containing all eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>There are a few reasons why we are interested in computing
eigenvectors and eigenvalues of a matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ol class="arabic simple">
<li><p>Eigenvalues and eigenvectors encode information about <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>Eigenvalues play an important role in stability calculations
in physics and engineering.</p></li>
<li><p>We can use eigenvectors to underpin the solution of linear systems
involving <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>…</p></li>
</ol>
<div class="section" id="how-to-find-eigenvalues">
<h2><span class="section-number">5.1. </span>How to find eigenvalues?<a class="headerlink" href="#how-to-find-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>The method that we first encounter in our mathematical education is to
find solutions of <span class="math notranslate nohighlight">\((A-\lambda I)x = 0\)</span>, which implies that
<span class="math notranslate nohighlight">\(\det(A-\lambda I)=0\)</span>. This gives a degree <span class="math notranslate nohighlight">\(m\)</span> polynomial to solve for
<span class="math notranslate nohighlight">\(\lambda\)</span>, called the <em>characteristic polynomial</em>. Unfortunately,
there is no general solution for polynomials of degree 5 or greater
(from Galois theory). Further, the problem of finding roots of
polynomials is numerically unstable. All of this means that we should
avoid using polynomials finding eigenvalues. Instead, we should try to
apply transformations to the matrix <span class="math notranslate nohighlight">\(A\)</span> to a form that means that the
eigenvalues can be directly extracted.</p>
<p>The eigenvalue decomposition of a matrix <span class="math notranslate nohighlight">\(A\)</span> finds a nonsingular matrix
<span class="math notranslate nohighlight">\(X\)</span> and a diagonal matrix <span class="math notranslate nohighlight">\(\Lambda\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[A = X\Lambda X^{-1}.\]</div>
</div></blockquote>
<p>The diagonal entries of <span class="math notranslate nohighlight">\(\Lambda\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>. Hence,
if we could find the eigenvalue decomposition of <span class="math notranslate nohighlight">\(A\)</span>, we could just
read off the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>; the eigenvalue decomposition is
“eigenvalue revealing”. Unfortunately, it is not always easy or even
possible to transform to an eigenvalue decomposition. Hence we shall
look into some other eigenvalue revealing decompositions of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>We quote the following result that explains when an eigenvalue
decomposition can be found.</p>
<div class="proof proof-type-theorem" id="id1">

    <div class="proof-title">
        <span class="proof-type">Theorem 5.1</span>
        
    </div><div class="proof-content">
<p>An <span class="math notranslate nohighlight">\(m\times m\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> has an eigenvalue decomposition if and
only if it is non-defective, meaning that the geometric
multiplicity of each eigenvalue (the dimension of the eigenspace
for that eigenvalue) is equal to the algebraic multiplicity (the
number of times that the eigenvalue is repeated as a root in the
characteristic polynomial <span class="math notranslate nohighlight">\(\det(I\lambda - A)=0\)</span>.</p>
</div></div><p>If the algebraic multiplicity is greater than the geometric
multiplicity for any eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>, then the matrix is defective,
the eigenspaces do not span <span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>, and an eigenvalue
decomposition is not possible.</p>
<p>This all motivates the search for other eigenvalue revealing
decompositions of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="proof proof-type-definition" id="id2">

    <div class="proof-title">
        <span class="proof-type">Definition 5.2</span>
        
            <span class="proof-title-name">(Similarity transformations)</span>
        
    </div><div class="proof-content">
<p>For <span class="math notranslate nohighlight">\(X\in \mathbb{C}^{m\times m}\)</span> a nonsingular matrix, the map
<span class="math notranslate nohighlight">\(A\mapsto X^{-1}AX\)</span> is called a similarity transformation of <span class="math notranslate nohighlight">\(A\)</span>.
Two matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are <em>similar</em> if <span class="math notranslate nohighlight">\(B=X^{-1}AX\)</span>.</p>
</div></div><p>The eigenvalue decomposition shows that (when it exists), <span class="math notranslate nohighlight">\(A\)</span> is similar
to <span class="math notranslate nohighlight">\(\Lambda\)</span>. The following result shows that it may be useful to examine
other similarity transformations.</p>
<div class="proof proof-type-theorem" id="id3">

    <div class="proof-title">
        <span class="proof-type">Theorem 5.3</span>
        
    </div><div class="proof-content">
<p>Two similar matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> have the same characteristic polynomial,
eigenvalues, and geometric multiplicities.</p>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>See a linear algebra textbook.</p>
</div></div><p>The goal is to find a similarity transformation such that <span class="math notranslate nohighlight">\(A\)</span> is
transformed to a matrix <span class="math notranslate nohighlight">\(B\)</span> that has some simpler structure where the
eigenvalues can be easily computed (with the diagonal matrix of the
eigenvalue decomposition being one example).</p>
<p>One such transformation comes from the Schur factorisation.</p>
<div class="proof proof-type-definition" id="id4">

    <div class="proof-title">
        <span class="proof-type">Definition 5.4</span>
        
            <span class="proof-title-name">(Schur factorisation)</span>
        
    </div><div class="proof-content">
<p>A Schur factorisation of a square matrix <span class="math notranslate nohighlight">\(A\)</span> takes the form <span class="math notranslate nohighlight">\(A =
QTQ^*\)</span>, where <span class="math notranslate nohighlight">\(Q\)</span> is unitary (and hence <span class="math notranslate nohighlight">\(Q^*=Q^{-1}\)</span>) and <span class="math notranslate nohighlight">\(T\)</span> is
upper triangular.</p>
</div></div><p>It turns out that, unlike the situation for the eigenvalue
decomposition, the following is true.</p>
<div class="proof proof-type-theorem" id="id5">

    <div class="proof-title">
        <span class="proof-type">Theorem 5.5</span>
        
    </div><div class="proof-content">
<p>Every square matrix has a Schur factorisation.</p>
</div></div><p>This is useful, because the characteristic polynomial of an upper
triangular matrix is just <span class="math notranslate nohighlight">\(\prod_{i=1}^m (\lambda-T_{ii})\)</span>, i.e.  the
eigenvalues of <span class="math notranslate nohighlight">\(T\)</span> are the diagonal entries
<span class="math notranslate nohighlight">\((T_{11},T_{22},\ldots,T_{mm})\)</span>. So, if we can compute the Schur
factorisation of <span class="math notranslate nohighlight">\(A\)</span>, we can just read the eigenvalues from the diagonal
matrices of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>There is a special case of the Schur factorisation, called the unitary
diagonalisation</p>
<div class="proof proof-type-definition" id="id6">

    <div class="proof-title">
        <span class="proof-type">Definition 5.6</span>
        
            <span class="proof-title-name">(Unitary diagonalisation)</span>
        
    </div><div class="proof-content">
<p>A unitary diagonalisation of a square matrix <span class="math notranslate nohighlight">\(A\)</span> takes the form <span class="math notranslate nohighlight">\(A =
Q\Lambda Q^*\)</span>, where <span class="math notranslate nohighlight">\(Q\)</span> is unitary (and hence <span class="math notranslate nohighlight">\(Q^*=Q^{-1}\)</span>) and <span class="math notranslate nohighlight">\(\Lambda\)</span>
is diagonal.</p>
</div></div><p>A unitary diagonalisation is a Schur factorisation <em>and</em> an eigenvalue
decomposition.</p>
<div class="proof proof-type-theorem" id="id7">

    <div class="proof-title">
        <span class="proof-type">Theorem 5.7</span>
        
    </div><div class="proof-content">
<p>A Hermitian matrix is unitary diagonalisable, with real <span class="math notranslate nohighlight">\(\Lambda\)</span>.</p>
</div></div><p>Hence, if we have a Hermitian matrix, we can follow a Schur
factorisation strategy (such as we shall develop in this section), and
obtain an eigenvalue decomposition as a bonus.</p>
</div>
<div class="section" id="transformations-to-schur-factorisation">
<h2><span class="section-number">5.2. </span>Transformations to Schur factorisation<a class="headerlink" href="#transformations-to-schur-factorisation" title="Permalink to this headline">¶</a></h2>
<p>Just as for the QR factorisations, we will compute the Schur
factorisation successively, with multiplication by a sequence of
unitary matrices <span class="math notranslate nohighlight">\(Q_1,Q_2,\ldots\)</span>. There are two differences for the
Schur factorisation. First, the matrices must be multiplied not just
on the left but also on the right with the inverse, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[A \mapsto \underbrace{Q_1^*AQ_1}_{A_1} \mapsto \underbrace{Q_2^*Q_1^*AQ_2Q_1}_{A_2}, \ldots\]</div>
</div></blockquote>
<p>At each stage, we have a similarity transformation,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[A = \underbrace{Q_1Q_2\ldots Q_k}_{=Q}A_k\underbrace{Q_k^*\ldots Q_2^*Q_1^*}_{=Q^*},\]</div>
</div></blockquote>
<p>emph{i.e.} <span class="math notranslate nohighlight">\(A\)</span> is similar to <span class="math notranslate nohighlight">\(A_k\)</span>. Second, the successive sequence is
infinite, i.e. we will develop an iterative method that converges in
the limit <span class="math notranslate nohighlight">\(k\to\infty\)</span>.  We should terminate the iterative method
when <span class="math notranslate nohighlight">\(A_k\)</span> is sufficiently close to being upper triangular (which</p>
<p>We should not be surprised by this news, since if the successive
sequence were finite, we would have derived an explicit formula for
computing the eigenvalues of the characteristic polynomial of <span class="math notranslate nohighlight">\(A\)</span>
which is explicit in general.</p>
<p>In fact, there are two stages to this process. The first stage, which
is finite (takes <span class="math notranslate nohighlight">\(m-1\)</span> steps) is to use similarity transformations to
upper Hessenberg form (<span class="math notranslate nohighlight">\(H_{ij}=0\)</span> for <span class="math notranslate nohighlight">\(i&gt;j+1\)</span>). If <span class="math notranslate nohighlight">\(A\)</span> is Hermitian,
then <span class="math notranslate nohighlight">\(H\)</span> will be tridiagonal. This stage is not essential but it makes
the second, iterative, stage much faster.</p>
</div>
<div class="section" id="similarity-transformation-to-upper-hessenberg-form">
<h2><span class="section-number">5.3. </span>Similarity transformation to upper Hessenberg form<a class="headerlink" href="#similarity-transformation-to-upper-hessenberg-form" title="Permalink to this headline">¶</a></h2>
<p>We already know how to use a unitary matrix to set all entries to zero
below the diagonal in the first column of <span class="math notranslate nohighlight">\(A\)</span> by left multiplication
<span class="math notranslate nohighlight">\(Q^*_1A\)</span>, because this is the Householder algorithm. The problem is
that we then have to right multiply by <span class="math notranslate nohighlight">\(Q_1\)</span> to make it a similarity
transformation, and this puts non-zero entries back in the column
again. The easiest way to see this is to write
<span class="math notranslate nohighlight">\(Q_1^*AQ_1=(Q_1^*(Q_1^*A)^*)^*\)</span>. <span class="math notranslate nohighlight">\((Q_1^*A)^*\)</span> has zeros in the first
row to the right of the first entry. Then, <span class="math notranslate nohighlight">\(Q_1^*(Q_1^*A)\)</span> creates
linear combinations of the first column with the other columns,
filling the zeros in with non-zero values again. Then finally taking
the adjoint doesn’t help with these non-zero values. Again, we
shouldn’t be surprised that this is impossible, because if it was,
then we would be able to build a finite procedure for computing
eigenvalues of the characteristic polynomial, which is impossible in
general.</p>
<p>A slight modification of this idea (and the reason that we can
transform to upper Hessenberg form) is to use a Householder rotation
<span class="math notranslate nohighlight">\(Q_1^*\)</span> to set all entries to zero below the <em>second</em> entry in the
first column. This matrix leaves the first row unchanged, and hence
right multiplication by <span class="math notranslate nohighlight">\(Q_1\)</span> leaves the first column unchanged. We
can create zeros using <span class="math notranslate nohighlight">\(Q_1^*\)</span> and <span class="math notranslate nohighlight">\(Q_1\)</span> will not destroy them. This
procedure is then repeated with multiplication by <span class="math notranslate nohighlight">\(Q_2^*\)</span>, which
leaves the first two rows unchanged and puts zeros below the third
entry in the second column, which are not spoiled by right
multiplication by <span class="math notranslate nohighlight">\(Q_2\)</span>. Hence, we can transform <span class="math notranslate nohighlight">\(A\)</span> to a similar
upper Hessenberg matrix <span class="math notranslate nohighlight">\(H\)</span> in <span class="math notranslate nohighlight">\(m-2\)</span> iterations.</p>
<p>This reduction to Hessenberg form can be expressed in the following
pseudo-code.</p>
<ul class="simple">
<li><p>FOR <span class="math notranslate nohighlight">\(k=1\)</span> TO <span class="math notranslate nohighlight">\(m-2\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x\gets A_{k+1:m,k}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_k\gets \mbox{sign}(x_1)\|x\|_2e_1 + x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_k\gets v_k/\|v\|_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A_{k+1:m,k:m} \gets A_{k+1:m,k:m}- 2v_k(v_k^*A_{k+1:m,k:m})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A_{k:m,k+1:m} \gets A_{k:m,k+1:m}- 2(A_{k:m,k+1:m,k:m}v_k)v_k^*\)</span></p></li>
</ul>
</li>
<li><p>END FOR</p></li>
</ul>
<p>Note the similarities and differences with the Householder algorithm
for computing the QR factorisation.</p>
<p>To calculate the operation count, we see that the algorithm is
dominated by the two updates to <span class="math notranslate nohighlight">\(A\)</span>, the first of which applies a
Householder reflection to rows from the left, and the second applies
the same reflections to columns from the right.</p>
<p>The left multiplication applies a Householder
reflection to the last <span class="math notranslate nohighlight">\(m-k\)</span> rows, requiring 4 FLOPs per
entry. However, these rows are zero in the first <span class="math notranslate nohighlight">\(k-1\)</span> columns,
so we can skip these and just work on the last <span class="math notranslate nohighlight">\(m-k+1\)</span> entries
of each of these rows.</p>
<p>Then, the total operation count for the left multiplication is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[4 \times \sum_{k=1}^{m-1} (m-k)(m-k+1) \sim \frac{4}{3}m^3.\]</div>
</div></blockquote>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Colin J. Cotter.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>