
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>1. Preliminaries &#8212; Computational linear algebra course 2020.0 documentation</title>
    <link rel="stylesheet" href="_static/fenics.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/proof.js"></script>
    <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. QR Factorisation" href="L2_QR_factorisation.html" />
    <link rel="prev" title="Computational Linear Algebra" href="index.html" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

<link rel="stylesheet" href="_static/featured.css">


<link rel="shortcut icon" href="_static/icon.ico" />


  </head><body>
<div class="wrapper">
  <a href="index.html"><img src="_static/banner.png" width="900px" alt="Project Banner" /></a>
  <div id="access">
    <div class="menu">
      <ul>
          <li class="page_item"><a href="https://github.com/Computational-Linear-Algebra-Course/computational-linear-algebra-course" title="GitHub">GitHub</a></li>
      </ul>
    </div><!-- .menu -->
  </div><!-- #access -->
</div><!-- #wrapper -->


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="preliminaries">
<h1><span class="section-number">1. </span>Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h1>
<p>In this preliminary section we revise a few key linear algebra
concepts that will be used in the rest of the course, emphasising
the column space of matrices. We will quote some standard results
that should be found in an undergraduate linear algebra course.</p>
<div class="section" id="matrices-vectors-and-matrix-vector-multiplication">
<h2><span class="section-number">1.1. </span>Matrices, vectors and matrix-vector multiplication<a class="headerlink" href="#matrices-vectors-and-matrix-vector-multiplication" title="Permalink to this headline">¶</a></h2>
<p>We will consider the multiplication of a vector</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}x = \begin{pmatrix} x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}, \quad x_i \in \mathbb{C}, \, i=1,2,\ldots,n,
\mbox{ i.e. } x \in \mathbb{C}^n,\end{split}\]</div>
</div></blockquote>
<p>by a matrix</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \ldots &amp; a_{mn} \\
\end{pmatrix},\end{split}\]</div>
</div></blockquote>
<p>i.e. <span class="math notranslate nohighlight">\(A\in \mathbb{C}^{m\times n}\)</span>. <span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(m\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns
so that the product</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b = Ax\]</div>
</div></blockquote>
<p>produces <span class="math notranslate nohighlight">\(b \in \mathbb{C}^m\)</span>, defined by</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b_i = \sum_{j=1}^n a_{ij}x_j, \, i=1,2,\ldots,m.\]</div>
</div></blockquote>
<p>In this course it is important to
consider the general case where <span class="math notranslate nohighlight">\(m \neq n\)</span>, which has many applications
in data analysis, curve fitting etc. We will usually state generalities
in this course for vectors over the field <span class="math notranslate nohighlight">\(\mathbb{C}\)</span>, noting where things
specialise to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>.</p>
<p>We can quickly check that the map <span class="math notranslate nohighlight">\(x \to Ax\)</span> given by matrix
multiplication is a linear map from <span class="math notranslate nohighlight">\(\mathbb{C}^n \to \mathbb{C}^m\)</span>, since
it is straightforward to check from the definition that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[A(\alpha x + y) = \alpha Ax + Ay,\]</div>
</div></blockquote>
<p>for all <span class="math notranslate nohighlight">\(x,y \in \mathbb{C}^n\)</span> and <span class="math notranslate nohighlight">\(\alpha\in \mathbb{C}\)</span>. (Exercise:
show this for yourself.)</p>
<p>It is very useful to interpret matrix-vector multiplication as a linear
combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> with coefficients taken from the entries
of <span class="math notranslate nohighlight">\(x\)</span>. If we write <span class="math notranslate nohighlight">\(A\)</span> in terms of the columns,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}
a_1 &amp; a_2 &amp; \ldots &amp; a_n \\
\end{pmatrix},\end{split}\]</div>
</div></blockquote>
<p>where</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[a_i \in \mathbb{C}^m, \, i=1,2,\ldots,n,\]</div>
</div></blockquote>
<p>then</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b = \sum_{j=1}^n x_j a_j,\]</div>
</div></blockquote>
<p>i.e. a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> as described above.</p>
<p>We can extend this idea to matrix-matrix multiplication. Taking
<span class="math notranslate nohighlight">\(A\in \mathbb{C}^{m\times l}\)</span>, <span class="math notranslate nohighlight">\(C\in \mathbb{C}^{l\times n}\)</span>,
<span class="math notranslate nohighlight">\(B\in \mathbb{C}^{m\times n}\)</span>, with <span class="math notranslate nohighlight">\(B=AC\)</span>, then the components of
<span class="math notranslate nohighlight">\(B\)</span> are given by</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b_{ij} = \sum_{k=1}^l a_{ik}c_{kj}, \quad 1\leq i \leq m, \,
1\leq j \leq n.\]</div>
</div></blockquote>
<p>Writing <span class="math notranslate nohighlight">\(b_j \in \mathbb{C}^m\)</span> as the jth column of <span class="math notranslate nohighlight">\(B\)</span>, for <span class="math notranslate nohighlight">\(1\leq j \leq n\)</span>,
and <span class="math notranslate nohighlight">\(c_j\)</span> as the jth column of <span class="math notranslate nohighlight">\(C\)</span>,
we see that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b_j = Ac_j.\]</div>
</div></blockquote>
<p>This means that the jth column of <span class="math notranslate nohighlight">\(B\)</span> is the matrix-vector product of
<span class="math notranslate nohighlight">\(A\)</span> with the jth column of <span class="math notranslate nohighlight">\(C\)</span>. This kind of “column thinking” is very
useful in understanding computational linear algebra algorithms.</p>
<p>An important example is the outer product of two vectors, <span class="math notranslate nohighlight">\(u \in
\mathbb{C}^m\)</span> and <span class="math notranslate nohighlight">\(v \in \mathbb{C}^n\)</span>. Here it is useful to see these
vectors as matrices with one column, i.e. <span class="math notranslate nohighlight">\(u \in \mathbb{C}^{m\times
1}\)</span> and <span class="math notranslate nohighlight">\(v \in \mathbb{C}^{n\times 1}\)</span>. The outer product is <span class="math notranslate nohighlight">\(u v^T
\in \mathbb{C}^{m\times n}\)</span>. The columns of <span class="math notranslate nohighlight">\(v^T\)</span> are just single numbers
(i.e. vectors of length 1), so viewing this as a matrix multiplication
we see</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[uv^T = \begin{pmatrix}
uv_1 &amp; uv_2 &amp; \ldots &amp; uv_n
\end{pmatrix},\]</div>
</div></blockquote>
<p>which means that all the columns of <span class="math notranslate nohighlight">\(uv^T\)</span> are multiples of <span class="math notranslate nohighlight">\(u\)</span>. We will
see in the next section that this matrix has rank 1.</p>
</div>
<div class="section" id="range-nullspace-and-rank">
<h2><span class="section-number">1.2. </span>Range, nullspace and rank<a class="headerlink" href="#range-nullspace-and-rank" title="Permalink to this headline">¶</a></h2>
<p>In this section we’ll quickly rattle through some definitions and results.</p>
<div class="proof proof-type-definition" id="id3">

    <div class="proof-title">
        <span class="proof-type">Definition 1.1</span>
        
            <span class="proof-title-name">(Range)</span>
        
    </div><div class="proof-content">
<p>The range of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\mbox{range}(A)\)</span>, is the set of vectors that can
be expressed as <span class="math notranslate nohighlight">\(Ax\)</span> for some <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div></div><p>The next theorem follows as a result of the column space
interpretation of matrix-vector multiplication.</p>
<div class="proof proof-type-theorem" id="id4">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.2</span>
        
    </div><div class="proof-content">
<p><span class="math notranslate nohighlight">\(\mbox{range}(A)\)</span> is the vector space spanned by the columns of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div></div><div class="proof proof-type-definition" id="id5">

    <div class="proof-title">
        <span class="proof-type">Definition 1.3</span>
        
            <span class="proof-title-name">(Nullspace)</span>
        
    </div><div class="proof-content">
<p>The nullspace <span class="math notranslate nohighlight">\(\mbox{null}(A)\)</span> of <span class="math notranslate nohighlight">\(A\)</span> (or kernel) is the set of
vectors <span class="math notranslate nohighlight">\(x\)</span> satisfying <span class="math notranslate nohighlight">\(Ax=0\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[\mbox{null}(A) = \{x \in \mathbb{C}^n: Ax=0\}.\]</div>
</div></div><div class="proof proof-type-definition" id="id6">

    <div class="proof-title">
        <span class="proof-type">Definition 1.4</span>
        
            <span class="proof-title-name">(Rank)</span>
        
    </div><div class="proof-content">
<p>The rank <span class="math notranslate nohighlight">\(\mbox{rank}(A)\)</span> of <span class="math notranslate nohighlight">\(A\)</span>
is the dimension of the column space
of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div></div><p>If</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}
a_1 &amp; a_2 &amp; \ldots &amp; a_n \\
\end{pmatrix},\end{split}\]</div>
</div></blockquote>
<p>the column space of <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\mbox{span}(a_1,a_2,\ldots,a_n)\)</span>.</p>
<div class="proof proof-type-definition" id="id7">

    <div class="proof-title">
        <span class="proof-type">Definition 1.5</span>
        
    </div><div class="proof-content">
<p>An <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> is full rank if it has maximum possible rank
i.e. rank equal to <span class="math notranslate nohighlight">\(\min(m, n)\)</span>.</p>
</div></div><p>If <span class="math notranslate nohighlight">\(m\geq n\)</span> then <span class="math notranslate nohighlight">\(A\)</span> must have <span class="math notranslate nohighlight">\(n\)</span> linearly independent columns to be
full rank. The next theorem is then a consequence of the column space
interpretation of matrix-vector multiplication.</p>
<div class="proof proof-type-theorem" id="id8">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.6</span>
        
    </div><div class="proof-content">
<p>An <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> is full rank if and only if it maps no two
distinct vectors to the same vector.</p>
</div></div><div class="proof proof-type-definition" id="id9">

    <div class="proof-title">
        <span class="proof-type">Definition 1.7</span>
        
    </div><div class="proof-content">
<p>A matrix <span class="math notranslate nohighlight">\(A\)</span> is called nonsingular, or invertible, if it is a square
matrix (<span class="math notranslate nohighlight">\(m=n\)</span>) of full rank.</p>
</div></div></div>
<div class="section" id="invertibility-and-inverses">
<h2><span class="section-number">1.3. </span>Invertibility and inverses<a class="headerlink" href="#invertibility-and-inverses" title="Permalink to this headline">¶</a></h2>
<p>This means that an invertible matrix has columns that form a basis for
<span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>. Given the canonical basis vectors defined by</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}e_j = \begin{pmatrix}
0 \\
\ldots \\
0 \\
1 \\
0 \\
\ldots \\
0 \\
\end{pmatrix},\end{split}\]</div>
</div></blockquote>
<p>i.e. <span class="math notranslate nohighlight">\(e_j\)</span> has all entries zero except for the jth entry which is 1, we can
write</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[e_j = \sum_{k=1}^m z_{ik} a_k, \quad 1\leq j \leq m.\]</div>
</div></blockquote>
<p>In other words,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}I =
\begin{pmatrix}
e_1 &amp; e_2 &amp; \ldots &amp; e_m
\end{pmatrix}\\= ZA.\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>We call <span class="math notranslate nohighlight">\(Z\)</span> a (left) inverse of <span class="math notranslate nohighlight">\(A\)</span>. (Exercises: show that <span class="math notranslate nohighlight">\(Z\)</span> is
the unique left inverse of <span class="math notranslate nohighlight">\(A\)</span>, and show that <span class="math notranslate nohighlight">\(Z\)</span> is also the unique
right inverse of <span class="math notranslate nohighlight">\(A\)</span>, satisfying <span class="math notranslate nohighlight">\(I = AZ\)</span>.) We write <span class="math notranslate nohighlight">\(Z=A^{-1}\)</span>.</p>
<p>The first four parts of the next theorem are a consequence of what
we have so far, and we shall quote the rest (see a linear algebra
course).</p>
<div class="proof proof-type-theorem" id="id10">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.8</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A \in \mathbb{C}^{m\times m}\)</span>. Then the following are equivalent.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> has an inverse.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{rank}(A)=m\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{range}(A)=\mathbb{C}^m\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{null}(A)=\{0\}\)</span>.</p></li>
<li><p>0 is not an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>0 is not a singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>The determinant <span class="math notranslate nohighlight">\(\det(A)\neq 0\)</span>.</p></li>
</ol>
</div></div><p>Finding the inverse of a matrix can be seen as a change of basis. Considering
the equation <span class="math notranslate nohighlight">\(Ax= b\)</span>, we have <span class="math notranslate nohighlight">\(x = A^{-1}b\)</span> for invertible <span class="math notranslate nohighlight">\(A\)</span>. We have
seen already that <span class="math notranslate nohighlight">\(b\)</span> can be written as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[b = \sum_{j=1}^m x_j a_j.\]</div>
</div></blockquote>
<p>Since the columns of <span class="math notranslate nohighlight">\(A\)</span> span <span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>, the entries of <span class="math notranslate nohighlight">\(x\)</span> thus
provide the unique expansion of <span class="math notranslate nohighlight">\(b\)</span> in the columns of <span class="math notranslate nohighlight">\(A\)</span> which form a
basis.  Hence, whilst the entries of <span class="math notranslate nohighlight">\(b\)</span> give basis coefficients for
<span class="math notranslate nohighlight">\(b\)</span> in the canonical basis <span class="math notranslate nohighlight">\((e_1,e_2,\ldots,e_m)\)</span>, the entries of <span class="math notranslate nohighlight">\(x\)</span>
give basis coefficients for <span class="math notranslate nohighlight">\(b\)</span> in the basis given by the columns of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
<div class="section" id="orthogonal-vectors-and-orthogonal-matrices">
<h2><span class="section-number">1.4. </span>Orthogonal vectors and orthogonal matrices<a class="headerlink" href="#orthogonal-vectors-and-orthogonal-matrices" title="Permalink to this headline">¶</a></h2>
<div class="proof proof-type-definition" id="id11">

    <div class="proof-title">
        <span class="proof-type">Definition 1.9</span>
        
            <span class="proof-title-name">(Adjoint)</span>
        
    </div><div class="proof-content">
<p>The adjoint (or Hermitian conjugate) of <span class="math notranslate nohighlight">\(A\in \mathbb{C}^{m\times n}\)</span>
is a matrix <span class="math notranslate nohighlight">\(A^* \in \mathbb{C}^{n\times m}\)</span> (sometimes written
<span class="math notranslate nohighlight">\(A^\dagger\)</span> or <span class="math notranslate nohighlight">\(A'\)</span>), with</p>
<div class="math notranslate nohighlight">
\[a^*_{ij} = \bar{a_{ji}},\]</div>
<p>where the bar denotes the complex conjugate of a complex number. If
<span class="math notranslate nohighlight">\(A^* = A\)</span> then we say that <span class="math notranslate nohighlight">\(A\)</span> is Hermitian.</p>
<p>For real matrices, <span class="math notranslate nohighlight">\(A^*=A^T\)</span>. If <span class="math notranslate nohighlight">\(A=A^T\)</span>, then we say that the matrix
is symmetric.</p>
</div></div><p>The following identity is very important when dealing with adjoints.</p>
<div class="proof proof-type-theorem" id="id12">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.10</span>
        
    </div><div class="proof-content">
<p>For matrices <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> with compatible dimensions (so that they can
be multiplied),</p>
<div class="math notranslate nohighlight">
\[(AB)^* = B^*A^*.\]</div>
</div></div></div>
<div class="section" id="inner-products-and-orthogonality">
<h2><span class="section-number">1.5. </span>Inner products and orthogonality<a class="headerlink" href="#inner-products-and-orthogonality" title="Permalink to this headline">¶</a></h2>
<p>The inner product is a critical tool in computational linear algebra.</p>
<div class="proof proof-type-definition" id="id13">

    <div class="proof-title">
        <span class="proof-type">Definition 1.11</span>
        
            <span class="proof-title-name">(Inner product)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x,y\in \mathbb{C}^m\)</span>. Then the inner product of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is</p>
<div class="math notranslate nohighlight">
\[x^*y = \sum_{i=1}^m \bar{x}_iy_i.\]</div>
</div></div><p>(Exercise: check that the inner product is bilinear, i.e. linear in
both of the arguments.)</p>
<p>We will frequently use the natural norm derived from the inner product
to define size of vectors.</p>
<div class="proof proof-type-definition" id="id14">

    <div class="proof-title">
        <span class="proof-type">Definition 1.12</span>
        
            <span class="proof-title-name">(2-Norm)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x\in \mathbb{C}^m\)</span>. Then the 2-norm of <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[\|x\| = \sqrt{\sum_{i=1}^m x_i^2} = \sqrt{x^*x}.\]</div>
</div></div><p>Orthogonality will emerge as an early key concept in this course.</p>
<blockquote>
<div><div class="proof proof-type-definition" id="id15">

    <div class="proof-title">
        <span class="proof-type">Definition 1.13</span>
        
            <span class="proof-title-name">(Orthogonal vectors)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x,y\in \mathbb{C}^m\)</span>. The two vectors are orthogonal if
<span class="math notranslate nohighlight">\(x^*y=0\)</span>.</p>
<p>Similarly, let <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> be two sets of vectors. The two sets
are orthogonal if</p>
<div class="math notranslate nohighlight">
\[x^*y = 0\, \forall x\in X, \, y\in Y.\]</div>
<p>A set <span class="math notranslate nohighlight">\(S\)</span> of vectors is itself orthogonal if</p>
<div class="math notranslate nohighlight">
\[x^*y = 0\,\forall x,y \in S.\]</div>
<p>We say that <span class="math notranslate nohighlight">\(S\)</span> is orthonormal if we also have <span class="math notranslate nohighlight">\(\|x\|=1\)</span>
for all <span class="math notranslate nohighlight">\(x\in S\)</span>.</p>
</div></div></div></blockquote>
</div>
<div class="section" id="orthogonal-components-of-a-vector">
<h2><span class="section-number">1.6. </span>Orthogonal components of a vector<a class="headerlink" href="#orthogonal-components-of-a-vector" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(S=\{q_1,q_2,\ldots,q_n\}\)</span> be an orthonormal set of vectors in
<span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>, and take another arbitrary vector <span class="math notranslate nohighlight">\(v\in \mathbb{C}^m\)</span>.
Now take</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[r = v - (q_1^*v)q_1 - (q_2^*v)q_2 - \ldots (q_n^*v)q_n.\]</div>
</div></blockquote>
<p>Then, we can check that <span class="math notranslate nohighlight">\(r\)</span> is orthogonal to <span class="math notranslate nohighlight">\(S\)</span>, by calculating
for each <span class="math notranslate nohighlight">\(1\leq i \leq n\)</span>,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[q^*_ir = q_i^*v - (q_1^*v)(q_i^*q_1) - \ldots (q_n^*v)(q_i^*q_n)\]</div>
<p>= q_i^*v - q_i^*v = 0,</p>
</div></blockquote>
<p>since <span class="math notranslate nohighlight">\(q_i^*q_j=0\)</span> if <span class="math notranslate nohighlight">\(i\neq j\)</span>, and 1 if <span class="math notranslate nohighlight">\(i=j\)</span>.
Thus,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[v = r + \sum_{i=1}^n (q_i^*v)q_i
= r + \sum_{i=1}^n \underbrace{(q_i q_i^*)}_{\mbox{rank-1 matrix}}v.\]</div>
</div></blockquote>
<p>If <span class="math notranslate nohighlight">\(S\)</span> is a basis for <span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>, then <span class="math notranslate nohighlight">\(n=m\)</span> and <span class="math notranslate nohighlight">\(r=0\)</span>, and we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[v = \sum_{i=1}^m (q_i q_i^*)v.\]</div>
</div></blockquote>
</div>
<div class="section" id="unitary-matrices">
<h2><span class="section-number">1.7. </span>Unitary matrices<a class="headerlink" href="#unitary-matrices" title="Permalink to this headline">¶</a></h2>
<div class="proof proof-type-definition" id="id16">

    <div class="proof-title">
        <span class="proof-type">Definition 1.14</span>
        
            <span class="proof-title-name">(Unitary matrices)</span>
        
    </div><div class="proof-content">
<p>A matrix <span class="math notranslate nohighlight">\(Q\in \mathbb{C}^{m\times m}\)</span> is unitary if <span class="math notranslate nohighlight">\(Q^* =Q^{-1}\)</span>.</p>
<p>For real matrices, a matrix <span class="math notranslate nohighlight">\(Q\)</span>  is orthogonal if <span class="math notranslate nohighlight">\(Q^T=Q^{-1}\)</span>.</p>
</div></div><div class="proof proof-type-theorem" id="id17">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.15</span>
        
    </div><div class="proof-content">
<p>The columns of a unitary matrix <span class="math notranslate nohighlight">\(Q\)</span> are orthonormal.</p>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>We have <span class="math notranslate nohighlight">\(I = Q^*Q\)</span>. Then using the column space interpretation
of matrix-matrix multiplication,</p>
<div class="math notranslate nohighlight">
\[e_j = Q^*q_j,\]</div>
<p>where <span class="math notranslate nohighlight">\(q_j\)</span> is the jth column of <span class="math notranslate nohighlight">\(Q\)</span>. Taking row i of <span class="math notranslate nohighlight">\(e_j\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_{ij} = q_i^*q_j, \mbox{ where }
\delta_{ij} = \left\{
\begin{array}{ccc}
1 &amp; \mbox{if} &amp; i=j, \\
0 &amp; \mbox{otherwise} &amp; \\
\end{array}\right. .\end{split}\]</div>
</div></div><p>Extending a theme from earlier, we can interpret <span class="math notranslate nohighlight">\(Q^*=Q^{-1}\)</span> as
representing a change of orthogonal basis. If <span class="math notranslate nohighlight">\(Qx = b\)</span>, then
<span class="math notranslate nohighlight">\(x=Q^*b\)</span> contains the coefficients of <span class="math notranslate nohighlight">\(b\)</span> expanded in the basis
given by the orthonormal columns of <span class="math notranslate nohighlight">\(Q\)</span>.</p>
</div>
<div class="section" id="vector-norms">
<h2><span class="section-number">1.8. </span>Vector norms<a class="headerlink" href="#vector-norms" title="Permalink to this headline">¶</a></h2>
<p>Various vector norms are useful to measure the size of a vector.
In computational linear algebra we need them for quantifying errors
etc.</p>
<div class="proof proof-type-definition" id="id18">

    <div class="proof-title">
        <span class="proof-type">Definition 1.16</span>
        
            <span class="proof-title-name">(Norms)</span>
        
    </div><div class="proof-content">
<p>A norm is a function <span class="math notranslate nohighlight">\(\|\cdot\|:\mathbb{C}^m \to \mathbb{R}\)</span>, such that</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\|x\|\geq 0\)</span>, and <span class="math notranslate nohighlight">\(\|x\|=0\implies x =0.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|x+y\| \leq \|x\| + \|y\|\)</span> (triangle inequality).</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\alpha x\| = |\alpha|\|x\|\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{C}^m\)</span>
and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{C}\)</span>.</p></li>
</ol>
</div></div><p>We have already seen the 2-norm, or Euclidean norm, which is part of a
larger class of norms called p-norms, with</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|x\|_p = \left(\sum_{i=1}^m |x_i|^p\right)^{1/p}, \quad\]</div>
</div></blockquote>
<p>for real ‘p&gt;0’. We will also consider weighted norms</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|x\|_{W,p} = \|Wx \|_p,\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is a matrix.</p>
</div>
<div class="section" id="projectors-and-projections">
<h2><span class="section-number">1.9. </span>Projectors and projections<a class="headerlink" href="#projectors-and-projections" title="Permalink to this headline">¶</a></h2>
<div class="proof proof-type-definition" id="id19">

    <div class="proof-title">
        <span class="proof-type">Definition 1.17</span>
        
            <span class="proof-title-name">(Projector)</span>
        
    </div><div class="proof-content">
<p>A projector <span class="math notranslate nohighlight">\(P\)</span> is a square matrix that satisfies <span class="math notranslate nohighlight">\(P^2=P\)</span>.</p>
</div></div><p>If <span class="math notranslate nohighlight">\(v \in \mbox{range}(P)\)</span>, then there exists <span class="math notranslate nohighlight">\(x\)</span> such that
<span class="math notranslate nohighlight">\(Pv = x\)</span>. Then,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Pv = P(Px) = P^2x = Px = v,\]</div>
</div></blockquote>
<p>and hence multiplying by <span class="math notranslate nohighlight">\(P\)</span> does not change <span class="math notranslate nohighlight">\(v\)</span>.</p>
<p>Now suppose that <span class="math notranslate nohighlight">\(Pv \neq v\)</span> (so that <a href="#id1"><span class="problematic" id="id2">`</span></a>vnotin mbox{range}(P)).
Then,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[P(Pv - v) = P^2v - Pv = Pv - Pv = 0,\]</div>
</div></blockquote>
<p>which means that <span class="math notranslate nohighlight">\(Pv-v\)</span> is the nullspace of <span class="math notranslate nohighlight">\(P\)</span>. We have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Pv -v = -(I-P)v.\]</div>
</div></blockquote>
<div class="proof proof-type-definition" id="id20">

    <div class="proof-title">
        <span class="proof-type">Definition 1.18</span>
        
            <span class="proof-title-name">(Complementary projector)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> be a projector. Then we call <span class="math notranslate nohighlight">\(I-P\)</span> the complementary projector.</p>
</div></div><p>To see that <span class="math notranslate nohighlight">\(I-P\)</span> is also a projector, we just calculate,</p>
<blockquote>
<div><blockquote>
<div><div class="math notranslate nohighlight">
\[(I-P)^2 = I^2 - 2P + P^2 = I - 2P + P = I - P.\]</div>
</div></blockquote>
<p>If <span class="math notranslate nohighlight">\(Pu=0\)</span>, then <span class="math notranslate nohighlight">\((I-P)u = u\)</span>.</p>
<p>In other words, the nullspace
of <span class="math notranslate nohighlight">\(P\)</span> is contained in the range of <span class="math notranslate nohighlight">\(I-P\)</span>.</p>
<p>On the other hand, if <span class="math notranslate nohighlight">\(v\)</span> is in the range of <span class="math notranslate nohighlight">\(I-P\)</span>,  then</p>
</div></blockquote>
<p>there exists some <span class="math notranslate nohighlight">\(w\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[v = (I-P)w = w - Pw.\]</div>
</div></blockquote>
<p>We have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Pv = P(w-Pw) = Pw - P^2w = Pw - Pw = 0.\]</div>
</div></blockquote>
<p>Hence, the range of <span class="math notranslate nohighlight">\(I-P\)</span> is contained in the nullspace of <span class="math notranslate nohighlight">\(P\)</span>.
Combining these two results we see that the range of <span class="math notranslate nohighlight">\(I-P\)</span>
is equal to the nullspace of <span class="math notranslate nohighlight">\(P\)</span>. Since <span class="math notranslate nohighlight">\(P\)</span> is the complementary
projector to <span class="math notranslate nohighlight">\(I-P\)</span>, we can repeat the same argument to show
that the range of <span class="math notranslate nohighlight">\(P\)</span> is equal to the nullspace of <span class="math notranslate nohighlight">\(I-P\)</span>.</p>
<p>We see that a projector <span class="math notranslate nohighlight">\(P\)</span> separates <span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span> into two
subspaces, the nullspace of <span class="math notranslate nohighlight">\(P\)</span> and the range of <span class="math notranslate nohighlight">\(P\)</span>. In fact the
converse is also true: given two subspaces <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span>
of <span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span> with <span class="math notranslate nohighlight">\(S_1 \cap S_2 = \{0\}\)</span>, then there
exists a projector <span class="math notranslate nohighlight">\(P\)</span> whose range is <span class="math notranslate nohighlight">\(S_1\)</span> and whose nullspace
is <span class="math notranslate nohighlight">\(S_2\)</span>.</p>
<p>Now we introduce orthogonality into the concept of projectors.</p>
<div class="proof proof-type-definition" id="id21">

    <div class="proof-title">
        <span class="proof-type">Definition 1.19</span>
        
            <span class="proof-title-name">(Orthogonal projector)</span>
        
    </div><div class="proof-content">
<p><span class="math notranslate nohighlight">\(P\)</span> is an orthogonal projector if</p>
<div class="math notranslate nohighlight">
\[(Pv)^*(Pv-v) = 0, \, \forall v \in \mathbb{C}^m.\]</div>
</div></div><p>In this case, <span class="math notranslate nohighlight">\(P\)</span> separates the space into two orthogonal subspaces.</p>
</div>
<div class="section" id="constructing-orthogonal-projectors-from-sets-of-orthonormal-vectors">
<h2><span class="section-number">1.10. </span>Constructing orthogonal projectors from sets of orthonormal vectors<a class="headerlink" href="#constructing-orthogonal-projectors-from-sets-of-orthonormal-vectors" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\{q_1,\ldots,q_n\}\)</span> be an orthonormal set of vectors in
<span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>. We write</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{Q} = \begin{pmatrix}
q_1 &amp; q_2 &amp; \ldots &amp; q_n \\
\end{pmatrix}.\end{split}\]</div>
<p>Previously we showed that for any <span class="math notranslate nohighlight">\(v\in \mathbb{C}^m\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[v = \underbrace{r}_{\mbox{Orthogonal to column space of }\hat{Q}} +
\underbrace{\sum_{i=1}^n (q_iq^*_i)v}_{\mbox{in the column space of }\hat{Q}}.\]</div>
<p>Hence, the map</p>
<div class="math notranslate nohighlight">
\[v \mapsto Pv = \underbrace{\sum_{i=1}^n (q_iq^*_i)}_{=P}v,\]</div>
<p>is an orthogonal projector. In fact, <span class="math notranslate nohighlight">\(P\)</span> has very simple form.</p>
<div class="proof proof-type-theorem" id="id22">
<span id="orthogonal-projector"></span>
    <div class="proof-title">
        <span class="proof-type">Theorem 1.20</span>
        
    </div><div class="proof-content">
<p>The orthogonal projector <span class="math notranslate nohighlight">\(P\)</span> takes the form</p>
<div class="math notranslate nohighlight">
\[P = \hat{Q}\hat{Q}^*.\]</div>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>From the change of basis interpretation of multiplication by
<span class="math notranslate nohighlight">\(\hat{Q}^*\)</span>, the entries in <span class="math notranslate nohighlight">\(\hat{Q}^*v\)</span> gives coefficients of the
projection of <span class="math notranslate nohighlight">\(v\)</span> onto the column space of <span class="math notranslate nohighlight">\(\hat{Q}\)</span> when expanded
using the columns as a basis. Then, multiplication by <span class="math notranslate nohighlight">\(\hat{Q}\)</span>
gives the projection of <span class="math notranslate nohighlight">\(v\)</span> expanded again in the canonical basis.
Hence, multiplication by <span class="math notranslate nohighlight">\(\hat{Q}\hat{Q}^*\)</span> gives exactly the same
result as multiplication by the formula for <span class="math notranslate nohighlight">\(P\)</span> above.</p>
</div></div><p>This means that <span class="math notranslate nohighlight">\(\hat{Q}\hat{Q}^*\)</span> is an orthogonal projection onto
the range of <span class="math notranslate nohighlight">\(\hat{Q}\)</span>. The complementary projector is <span class="math notranslate nohighlight">\(P_{\perp} =
I - \hat{Q}\hat{Q}^*\)</span> is an orthogonal projection onto the nullspace
of <span class="math notranslate nohighlight">\(\hat{Q}\)</span>.</p>
<p>An important special case is when <span class="math notranslate nohighlight">\(\hat{Q}\)</span> has just one column,
and then</p>
<div class="math notranslate nohighlight">
\[P = q_1q_1^*, \, P_{\perp}=I - q_1q_1^*.\]</div>
<p>We notice that <span class="math notranslate nohighlight">\(P^* = (\hat{Q}\hat{Q}^*) = \hat{Q}\hat{Q}^* = P\)</span>.
In fact the following is true.</p>
<div class="proof proof-type-theorem" id="id23">

    <div class="proof-title">
        <span class="proof-type">Theorem 1.21</span>
        
    </div><div class="proof-content">
<p><span class="math notranslate nohighlight">\(P=P^*\)</span> if and only if <span class="math notranslate nohighlight">\(Q\)</span> is an orthogonal projector.</p>
</div></div></div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Colin J. Cotter.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>